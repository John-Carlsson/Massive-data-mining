{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## MinHash at work\n",
    "\n",
    "The libray 'datasketch' gives you probabilistic data structures that can process and search very large\n",
    "amount of data super fast, with little loss of accuracy.\n",
    "\n",
    "datasketch.MinHash lets you estimate the Jaccard similarity\n",
    "between sets of arbitrary sizes in linear time, O(m+n), using a small and fixed memory space.\n",
    "It can also be used to compute Jaccard similarity between data streams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasketch.minhash import MinHash\n",
    "from datasketch.lsh import MinHashLSH\n",
    "\n",
    "\"\"\"\n",
    "Datasketch uses shingles made of one word.\n",
    "\"\"\"\n",
    "\n",
    "document_1 = 'minhash is a probabilistic data structure for'\n",
    "document_1 += ' estimating the similarity between datasets'\n",
    "\n",
    "document_2 = 'minhash is a probability data structure for'\n",
    "document_2 += ' estimating the similarity between documents'\n",
    "\n",
    "document_3 = 'This is a completly different probabilistic thing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split creates a list from a string using white spaces as separator\n",
    "# the function set converts lists to sets\n",
    "\n",
    "use_shingles = False\n",
    "\n",
    "if use_shingles:\n",
    "    shing_len = 5\n",
    "    set_1 = set([document_1[i:i + shing_len] for i in range(len(document_1) - shing_len + 1)])\n",
    "    set_2 = set([document_2[i:i + shing_len] for i in range(len(document_2) - shing_len + 1)])\n",
    "    set_3 = set([document_3[i:i + shing_len] for i in range(len(document_3) - shing_len + 1)])\n",
    "else:\n",
    "    set_1 = set(document_1.split())\n",
    "    set_2 = set(document_2.split())\n",
    "    set_3 = set(document_3.split())\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "# compute and print Jaccard similarity between set_1 and set_2\n",
    "jd = len(set_1.intersection(set_2)) / len(set_1.union(set_2))\n",
    "print(\"Jaccard similarity between set_1 and set_2 is: %.2f\" % jd)\n",
    "\n",
    "# compute and print Jaccard similarity between set_1 and set_3\n",
    "jd = len(set_1.intersection(set_3)) / len(set_1.union(set_3))\n",
    "print(\"Jaccard similarity between set_1 and set_3 is: %.2f\" % jd)\n",
    "\n",
    "# compute and print Jaccard similarity between set_2 and set_3\n",
    "jd = len(set_2.intersection(set_3)) / len(set_2.union(set_3))\n",
    "print(\"Jaccard similarity between set_2 and set_3 is: %.2f\" % jd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_perm = 32 # try different permutations\n",
    "\n",
    "# Costruct the min hash of our reference document\n",
    "m1 = MinHash(num_perm=num_perm) # construct the object MinHash\n",
    "for s in set_1:\n",
    "    m1.update(s.encode('utf8')) # let's populate m1\n",
    "\n",
    "a = m1.digest() \n",
    "# extract into an numpy array the MinHash actual value\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Costruct the min hash of the other document to asses similarity\n",
    "m2 = MinHash(num_perm=num_perm) # new object\n",
    "m3 = MinHash(num_perm=num_perm) # new object\n",
    "# feed the min hashes with data of document 2 and 3\n",
    "for s in set_2:\n",
    "    m2.update(s.encode('utf8')) # populate\n",
    "for s in set_3:\n",
    "    m3.update(s.encode('utf8')) # populate\n",
    "\n",
    "print(\"Jaccard similarity between min hashes of set_1 and set_2 is: %.2f\" % m1.jaccard(m2))\n",
    "print(\"Jaccard similarity between min hashes of set_1 and set_3 is: %.2f\" % m1.jaccard(m3))\n",
    "print(\"Jaccard similarity between min hashes of set_2 and set_3 is: %.2f\" % m2.jaccard(m3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LSH index\n",
    "threshold = 0.6  # Jaccard similarity\n",
    "lsh = MinHashLSH(threshold=threshold, num_perm=num_perm) # construct the object LSH\n",
    "lsh.insert(\"m2\", m2) # let's populate\n",
    "lsh.insert(\"m3\", m3) # let's populate\n",
    "#lsh.insert(\"m1\", m1) # let's populate\n",
    "\n",
    "result = lsh.query(m1) # is m1 similar to m2 or m3\n",
    "print(\"Approximate neighbours of m1 with Jaccard similarity >= %f\" % threshold, result)\n",
    "\n",
    "\n",
    "# excercise\n",
    "\n",
    "# add 3 or more string documents with various level of similarities and rerun the query. Observation.\n",
    "# add 3 or more documents with lot of typo errors and rerun similarity query. Observations.\n",
    "# try with different shingles size.\n",
    "# try with different numbers of permutation. Observation.\n",
    "# print the difference vector of jaccard based on set and jaccard based on MinHashes for various permutations\n",
    "# Is mx.jaccard(my) equal to my.jaccard(mx)? Try them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinHash merging\n",
    "\n",
    "document = \"\"\"In computer science and data mining, MinHash \n",
    "(or the min-wise independent permutations locality sensitive hashing scheme) is a technique for quickly \n",
    "estimating how similar two sets are. The scheme was invented by Andrei Broder (1997),[1] and initially \n",
    "used in the AltaVista search engine to detect duplicate web pages and eliminate them from search results.\n",
    "It has also been applied in large-scale clustering problems, such as clustering documents by the similarity \n",
    "of their sets of words\"\"\"\n",
    "\n",
    "ds = document.split()\n",
    "half_low = ds[0:int(len(ds)/2)]\n",
    "half_up = ds[int(len(ds)/2):]\n",
    "\n",
    "half_up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mh_doc = MinHash(num_perm=num_perm) # construct the object MinHash\n",
    "for s in ds:\n",
    "    mh_doc.update(s.encode('utf8'))\n",
    "\n",
    "\n",
    "mh_low = MinHash(num_perm=num_perm)\n",
    "for s in half_low:\n",
    "    mh_low.update(s.encode('utf8'))\n",
    "\n",
    "\n",
    "mh_up = MinHash(num_perm=num_perm)\n",
    "for s in half_up:\n",
    "    mh_up.update(s.encode('utf8'))\n",
    "\n",
    "\n",
    "print(\"Jaccard similarity between min hashes of doc and half_low is: %.2f\" % mh_doc.jaccard(mh_low))\n",
    "print(\"Jaccard similarity between min hashes of doc and half_up is: %.2f\" % mh_doc.jaccard(mh_up))\n",
    "\n",
    "# excercise.\n",
    "# compute jaccard distance based on sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mh_low.merge(mh_up)\n",
    "print(\"Jaccard similarity between min hashes of doc and merge of low and up is: %.2f\" % mh_low.jaccard(mh_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
